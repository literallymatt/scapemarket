{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RuneScape Item Price Exploration\n",
    "\n",
    "This notebook is for exploring the raw price data collected from the OSRS Wiki API. The goal is to visualize the data, understand its properties, and perform the necessary preprocessing and feature engineering before applying machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (18, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Data\n",
    "\n",
    "First, we load the raw JSON data, parse it into a pandas DataFrame, and perform initial cleaning steps like converting the timestamp and setting it as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = Path('../data/raw')\n",
    "ITEM_FILE = RAW_DATA_PATH / 'twisted_bow_1h.json'\n",
    "\n",
    "if not ITEM_FILE.exists():\n",
    "    print(f\"File not found: {ITEM_FILE}\")\n",
    "    print(\"Please run `python -m src.main collect` from the root directory first.\")\n",
    "else:\n",
    "    # The API data is nested under a 'data' key\n",
    "    df_raw = pd.read_json(ITEM_FILE)\n",
    "    df = pd.json_normalize(df_raw['data'])\n",
    "    \n",
    "    # Convert unix timestamp to datetime and set as index\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df = df.set_index('timestamp')\n",
    "    \n",
    "    # Create a single average price column for simplicity\n",
    "    df['avgPrice'] = df[['avgHighPrice', 'avgLowPrice']].mean(axis=1).fillna(0)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values check:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nData Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nData Head:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Visualization\n",
    "\n",
    "A simple line plot is the best way to get a first look at the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    df['avgPrice'].plot(title='Twisted Bow Price Over Time', lw=2)\n",
    "    plt.ylabel('Price (GP)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now we create features that our model can use to make predictions. Raw price alone is not enough; we need to provide context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define the Target Variable\n",
    "\n",
    "The most important step is defining what we want to predict. A common goal in time series forecasting is to predict the next value. We'll create a `target` column that is the `avgPrice` of the *next* time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # Shift the price from the next period to the current row\n",
    "    df['target'] = df['avgPrice'].shift(-1)\n",
    "    \n",
    "    # The last row will have a NaN target, which is expected\n",
    "    display(df[['avgPrice', 'target']].head())\n",
    "    display(df[['avgPrice', 'target']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Rolling Features (Trend and Volatility)\n",
    "\n",
    "- **Rolling Mean (Moving Average):** This smooths out short-term fluctuations and helps identify longer-term trends. Crossovers between short-term and long-term moving averages are classic trading signals.\n",
    "- **Rolling Standard Deviation:** This measures volatility. High volatility means high risk and high potential reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # Using 1-day and 7-day windows for our 1-hour data\n",
    "    df['rolling_mean_24h'] = df['avgPrice'].rolling(window=24).mean()\n",
    "    df['rolling_mean_168h'] = df['avgPrice'].rolling(window=168).mean() # 168 hours = 7 days\n",
    "    \n",
    "    df['rolling_std_24h'] = df['avgPrice'].rolling(window=24).std()\n",
    "\n",
    "    # Plotting the rolling means\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df.index, df['avgPrice'], label='Average Price', color='lightblue', lw=1)\n",
    "    ax.plot(df.index, df['rolling_mean_24h'], label='24h Rolling Mean', color='orange', lw=2)\n",
    "    ax.plot(df.index, df['rolling_mean_168h'], label='7d Rolling Mean', color='red', lw=2)\n",
    "    ax.set_title('Price vs. Rolling Means')\n",
    "    ax.set_ylabel('Price (GP)')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Time-Based Features\n",
    "\n",
    "Player activity often follows weekly and daily cycles. For example, more players are online during evenings and weekends, which can affect prices. We can capture this by extracting features from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek # Monday=0, Sunday=6\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    display(df[['hour', 'day_of_week', 'is_weekend']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize if there's a weekly pattern. A box plot is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.boxplot(x='day_of_week', y='avgPrice', data=df, ax=ax)\n",
    "    ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "    ax.set_title('Average Price by Day of the Week')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Lag Features\n",
    "\n",
    "The price at time `t` is often highly correlated with the price at `t-1`, `t-2`, etc. We can provide these past values directly to the model as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    # We'll create lags for 1 hour, 24 hours (1 day), and 168 hours (1 week) ago\n",
    "    for lag in [1, 24, 168]:\n",
    "        df[f'lag_{lag}h'] = df['avgPrice'].shift(lag)\n",
    "        \n",
    "    display(df[['avgPrice', 'lag_1h', 'lag_24h']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Processed DataFrame\n",
    "\n",
    "After creating all these features, our DataFrame has many `NaN` (Not a Number) values at the beginning (from rolling features and lags) and one at the end (for the target). We must drop these rows before training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    print(f\"Shape before dropping NaNs: {df.shape}\")\n",
    "    df_processed = df.dropna()\n",
    "    print(f\"Shape after dropping NaNs:  {df_processed.shape}\")\n",
    "    \n",
    "    # Select only the features we will use for the model\n",
    "    # We exclude the original high/low prices and volumes\n",
    "    features = [\n",
    "        'avgPrice',\n",
    "        'rolling_mean_24h',\n",
    "        'rolling_mean_168h',\n",
    "        'rolling_std_24h',\n",
    "        'hour',\n",
    "        'day_of_week',\n",
    "        'day_of_year',\n",
    "        'is_weekend',\n",
    "        'lag_1h',\n",
    "        'lag_24h',\n",
    "        'lag_168h',\n",
    "        'target' # Keep the target column\n",
    "    ]\n",
    "    \n",
    "    df_final = df_processed[features]\n",
    "    \n",
    "    print(\"\\nFinal DataFrame Head:\")\n",
    "    display(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "This `df_final` DataFrame is now ready for machine learning!\n",
    "\n",
    "1.  **Formalize this logic:** Move the feature engineering steps from this notebook into a dedicated `src/feature_engineering.py` script.\n",
    "2.  **Split the data:** Separate the data into training, validation, and testing sets. **Crucially, for time series, this must be a chronological split, not a random one.**\n",
    "3.  **Train a model:** Start with a simple baseline like `LinearRegression` or a more powerful model like `XGBoost` to predict the `target` column using all other columns as features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
